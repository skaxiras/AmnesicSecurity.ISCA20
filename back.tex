\subsection{Speculation Shadows} 
Sakalis et al. introduced the concept of \textbf{\emph{Speculative Shadows}} to reason about the earliest time an instruction becomes non-speculative and is considered safe to execute regardless of its effects on $\mu$-architectural state~\cite{sakalis+:CF2019ghost,sakalis+:ISCA2019vp}.
%Instructions that can cast speculative shadows on younger instructions in the dynamic instruction stream are known as \textbf{speculation primitives}.
Speculative shadows can be of the following types: \emph{E-Shadows} are cast by any instruction that can cause an \textbf{exception}; 
\emph{C-Shadows} are cast by \textbf{control instructions}, such as branches and jumps, when either the branch condition or the target address are unknown or have been predicted but not yet verified; \emph{D-Shadows} are cast by potential \textbf{data dependencies} through stores with unresolved addresses (read-after-write dependencies); \emph{M-shadows} are cast by \textbf{speculatively executed memory accesses} that may be caught violating the ordering rules of a memory model (e.g., total store order---TSO) and therefore may need to be squashed; and \emph{VP-shadows} are cast by \textbf{value-predicted loads}~\cite{sakalis+:ISCA2019vp}. To be more specific, shadows demarcate regions of speculative instructions. So far, attacks have been demonstrated under the E-~\cite{lipp_meltdown_2018}, C-~\cite{kocher_spectre_2018}, and D-Shadows~\cite{CVE-2018-3693} only, but we cannot exclude future attacks using the rest.

\subsection{Threat Model}
\label{sec:threat}
%\input{threat} We do not need more sub-levels do we?
%{ \color{red} [FIXME: Probably discuss after the shadows?] }

We target speculative side- or covert-channel attacks that utilize the memory hierarchy (caches, directories, and the main memory) as their side-channel. Non-speculative cache side-channel attacks, as well as attacks that use other side-channels (such as port contention) are outside the scope of this work.
We make no assumptions as to where the attacker is located in relation to the victim (on the core) or if they share the same virtual memory address space or not.
As a matter of fact the attacker and the victim can be the same process, as in the Spectre v1 attack~\cite{kocher_spectre_2018}.
We assume that the attacker can execute arbitrary code or otherwise redirect the execution of running code arbitrarily.
How the attacker manages to execute or redirect such code is beyond the scope of this work. Instead of focusing on preventing the attacker from accessing data illegally, we focus on preventing the transmission of such data through a side- or covert-channel.

In this work, we use the concept of speculative shadows to determine when a load is safe or not. Speculative shadows determine the earliest point at which an instruction is guaranteed to be committed and retired successfully. Other works, such as InvisiSpec~\cite{yan_invisispec:MICRO2018} and NDA~\cite{weisse2019nda}, make different assumptions based on the threat model. For example, InvisiSpec provides two different versions, one based on the initial Spectre attacks where only the equivalent of C-Shadows are considered as part of the threat model, and one based on protecting against all possible future attacks, utilizing all the shadows. Similarly, NDA provides different solutions if only C-Shadows are considered (strict/permissive data propagation), if D-Shadows should also be considered (bypass restriction), or if all shadows should be considered (load restriction). In this work we assume that all shadows have the potential of being abused, as we cannot reasonably argue that any of them are not exploitable.

{\em To summarize, 
%similar to previous proposals, 
we cover any known or yet-to-be-discovered side-channel posed by a speculative memory read. We assume that all system components operate correctly.}
%-- including but not limited to Spectre (and alike)}. 

\subsection{Delay-on-Miss}
\label{sec:dom-vp}
%\input{dom-vp} We do not need more sub-levels do we?
The goal of Delay-on-Miss is to hide speculative changes in the memory hierarchy (including main memory). 
To achieve this, Delay-on-Miss delays speculative loads that miss in the L1 cache. Loads that hit in the L1 (and their dependent instructions) are allowed to execute speculatively as their effects (i.e., on the L1 replacement state) can be deferred for when the loads are cleared from any speculation shadow. 
The miss of a delayed load is allowed to be resolved in the memory hierarchy at the earliest point the load becomes non-speculative. An efficient mechanism to track shadows is proposed by Sakalis et al.~\cite{sakalis+:ISCA2019vp}. 

In Delay-on-Miss, the wast majority of all loads are executed speculatively (80+\% on average~\cite{sakalis+:ISCA2019vp}), which causes a notable fraction of the loads to be delayed. This takes up precious resources (i.e., entries in the instruction queue, the reorder buffer, and the load/store queue) and eventually stalls instructions from committing. 
%Similar issues arise with other ``delay'' approaches such as NDA~\cite{weisse2019nda} and STT~\cite{yu_speculative:MICRO2019-STT}.
The significant amount of speculation that is performed, results in each load being covered by several speculative shadows (five on average according to our simulations). This forces the majority of the loads to be executed serially, severely limiting MLP. Furthermore, removing any individual shadow (e.g., the M-Shadow) has a limited effect, as the load will be covered by another overlapping shadow.

\iffalse

{\color{red} TO BE REVISED

A critical factor that affects the performance of Delay-on-Miss (with respect to a non-delaying baseline) is its effect on MLP for the delayed misses. 
A stalled L1 miss is allowed to proceed (and affect changes in the $\mu$-architectural state) just after the \emph{first} load that caused the miss becomes non-speculative.
Many times, a single L1 miss satisfies multiple loads.\footnote{This property was exploited since the early high-performance architectures such as the Alpha 21164: each of its MSHRs could satisfy up to four outstanding loads that missed on the same cacheline~\cite{alpha21164}.} 
In such cases, MLP among loads to the same cacheline is attained effortlessly.

MLP, however, among loads on \emph{different} cachelines is more involved and depends on restrictions imposed by the memory consistency model (MCM). We will examine TSO and {\rc} as representative MCMs.

Assume that a delayed load exits all speculative shadows: E-Shadows (no instruction before it, or itself, will cause an exception), C-Shadow (all preceding branches have resolved), and D-Shadow (all preceding stores have resolved their addresses). The only shadow that is left is the M-Shadow that is governed by the MCM:
\squishlist
\item {\rc}: In {\rc} load$\rightarrow$load order is not enforced. An unperformed load does not cast an M-Shadow on younger loads, therefore there is no restriction on the MLP among delayed loads that are unshadowed.
\item TSO: In TSO load$\rightarrow$load order \emph{is} enforced and unperformed loads cast M-Shadows on younger loads, preventing them from releasing their L1 misses. %However, even in this case there is a way to remove the M-Shadow. The key is non-speculative load-load reordering proposed by Ros et al.~\cite{aros-isca17}. Assuming that a load is unshadowed by any other shadow other than the M-Shadow, the M-Shadow can be removed by making the younger loads non-speculative via the cache protocol~\cite{aros-isca17}. \emph{Under this perspective TSO is no different than {\rc} with regards to Delay-on-miss MLP.}
\squishend

%To conclude, one can argue that we do not need M-Shadows in any MCM: {\rc} does not have M-Shadows, and a TSO implementation that allows non-speculative load--load reordering~\cite{aros-isca17}, effectively eliminates the M-Shadows. 

%Non-speculative load--load reordering delays remote stores which can be construed as a side-channel of a similar threat level to an invalidation side-channel~\cite{}. However, we stress here that this behavior is not a \emph{speculative} side channel as it only takes effect for \emph{unshadowed} loads, after they release their misses to the memory system.

}

\fi

\subsection{Delay-on-Miss and Value Prediction} 

The concept behind using value prediction (VP) with Delay-on-Miss is to speed up the delayed loads (and their dependent instructions) and regain some of the lost performance. Unfortunately, in our implementation, VP---no mater how good we make it (even under 100\% coverage and accuracy)---gives only a limited benefit on top of Delay-on-Miss. VP clearly cannot regain the lost performance becuase of the following reasons:

%VP needs to be validated. In order! Non-speculative load-load reordering cannot hep here as we are bound by a new speculation: VP-speculation (VP-Shadow). This prevents any MLP for the validation phase. Whatever VP wins in latency (pre-executing load and dependent instructions if correct), looses later in MLP. Result: limited.

VP cannot help much as it simply provides values early; however, the validation is still delayed until all shadows have been lifted. Thus, precious core resources are still occupied until the same point in time as simply delaying the load.
The only perceptible difference is a faster commit of pre-executed dependent instructions if the validation of a value-predicted load proves to be correct.

Furthermore, VP introduces a new speculative shadow, which is referred to as the \emph{VP-Shadow}. This new shadow is only lifted from younger loads when the validation of the VP is complete. Thus, not only does VP occupy precious resources in the same manner as the Delay-on-Miss, but it further prevents younger loads from validating in parallel, limiting the MLP. 
%The astute reader will note that this is the same as the M-Shadow an unperformed load casts on younger loads. Therefore a V-Shadow should not matter in the cases where we do have M-Shadows as the two shadows would completely overlap. As we argued above, we can eliminate the M-Shadow (even in MCMs that enforce load$\rightarrow$load ordering) which leaves the \emph{V-Shadow} as a liability that detracts from MLP and performance.
%When VP coverage is fairly low, e.g., below 20\% as in~\cite{}, this effect is not pronounced, compared to the effect of removing the M-Shadow for all loads.

%\subsection{Value Recomputation vs. Value Prediction}
\subsection{Value Recomputation}
\label{sec:recmp}

Due to imbalances in technology scaling,  the energy usage (and latency) of data transfers in the memory hierarchy can easily exceed the energy usage (and latency) of value recomputation~\cite{Horow}.
Value recomputation (\recomp) is proposed as a way to trade off data movement in the memory hierarchy for in-core computation to save energy~\cite{amnesiac17,taxonomy18}.
%
The basic idea is to swap slow and energy-hungry loads for recomputation of the respective data values.  
This is achieved by identifying a slice of producer instructions of the respective data values and executing them when the value is needed.
Each such slice forms a backward slice of execution, and strictly contains only arithmetic and logic instructions. 
% No branch, load, or store can find place in slices. 
% Slices typically form short sequences (in the order of a few tens) of instructions. 
% The overhead of \recomp\ increases with slice length. 
% Therefore, \recomp\ cannot deliver any energy or latency benefit if this overhead exceeds the overhead of performing the actual load (which evolves depending on the level in the memory hierarchy to service the load).
%\recomp{} can deliver an energy and latency improvement if the cost of computing the slice (often consisting of a few tens of instructions) is less than the cost of performing the load (which depends on the cache level at which the data resides).

As depicted in Figure~\ref{fig:rt}, each slice represents a data-dependency graph, where nodes correspond to producer instructions to be (re)executed. Data flows from the parent nodes to their children. The root represents the producer of the store whose value will be recomputed when its corresponding load is encountered, i.e., a load accessing the same memory location.
Nodes at level 1 are immediate producers of the (input operands of the) root, nodes at level 2 are producers of nodes at level 1, and so on and so fort. 
The parents which do not have any producers are terminal instructions whose input operands must be available at the time of recomputation. 
\ignore{
Slices cannot grow indefinitely due to size limitations of  $\mu$-architectural buffers.
The overhead of recomputation includes the cost of retrieving
input operands of the leaf nodes (which cannot rely on producers to recompute their
inputs).
%
In the example from Figure~\ref{fig:rt}, P1 and P2 at level 1 correspond to
producers of $P(v)$'s input operands. (Re)execution of P1 does not require any
more (re)execution. (Re)execution of P2, on the other hand, requires the (re)execution
of three of P2's producers: P3, P4, and P5, respectively. The leaf producers are
all shaded in gray.  The leaves either represent terminal instructions which do
not have any producers (e.g., instructions with constants as input operands), or
instructions for which 
%loading their input operands 
(re)execution of their producers is not energy-efficient. Amnesic execution can
only function, if the input operands of leaf instructions are available at their
anticipated time of (re)execution.
}
If these input operands are read-only values to be loaded from memory (such as program inputs) or register values that will be overwritten then buffering of these values are need to enable recomputation of the load~\cite{amnesiac17}. See \autoref{sec:slice_formation} and \autoref{sec:iser-architecture} for more information regarding slice formation and the required architectural support.
%Buffering such inputs can help keeping the copy before being overwritten, and reducing the energy cost of their retrieval; however, no dedicated buffering is necessary, on the other hand, if slice inputs correspond to immediate\footnote{Immediate values are the constants encoded in the instruction itself, as in I-type instructions}  or live register values. 

\ignore{
At the same time, for correctly resuming the execution, the recomputation should not alter the architectural state (specifically, the register file).  
One way to achieve this is to checkpoint the register file before recomputation starts (and restore it back when recomputation finishes).
A more efficient approach is to deploy a scratchpad (similar to the physical register file in nature but much smaller) and to 
use the scratchpad as the equivalent of the physical register file during recomputation, while keeping the architectural register file intact. In this case the register references of slice
instructions need to be renamed to scratchpad entries, and the mechanism is not any different in principle than classical register renaming. 
}

{\noindent \bf Premise:}
\recomp\ has the potential to render a more energy efficient (and faster) execution than servicing a miss in the memory hierarchy. At the same time, there is no need for MLP, since as opposed to VP, \emph{\recomp{} is not speculative} by itself and does not require any costly validation. 
A recomputed load can be committed as soon as all the shadows are lifted---this is in stark contrast to Delay-on-Miss/VP, which require a load/validation to be performed before commit.